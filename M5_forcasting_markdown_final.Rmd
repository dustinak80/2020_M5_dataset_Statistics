---
title: "M5 Forecasting - Multivariate Analysis"
author: "Bryan Florence, Jordan Robles, Dustin Vasquez"
date: "4/14/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# GENERAL DATA MANIPULATION
library('dplyr') # data manipulation
library('readr') # input/output
# install.packages("vroom")
library('vroom') # input/output
# install.packages("skimr")
library('skimr') # overview
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('purrr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation
#install.packages('lubridate')
library('lubridate') # date and time

#PLOTTING
library(ggplot2)

#FOR RMARKDOWN
library('knitr')
```

```{r, echo=FALSE, warning=FALSE}
####  USE THIS CHUNK FOR FUNCTIONS

#Get the Stats
stats = function(df){
  features = sapply(df, is.numeric)

  Mean = sapply(df[features], mean)
  Std.Dev = sapply(df[features], sd)
  Min = sapply(df[features], min)
  Median = sapply(df[features], median)
  Max = sapply(df[features], max)

  stats = rbind(Mean, Std.Dev, Min, Median, Max)
}

#### PIVOT TABLE FOR TIME SERIES
extract_ts = function(data, column){
  # by stating the column, you are saying which columns you are wanting showed. Can
  # be multiple columns
  min_date = as.Date("2011-01-29", format = "%Y-%m-%d")

  data %>%
    select(column, starts_with('d_')) %>% #This grabs only the column in the select()
# (columns to pivot, name of column for pivoted columns, name of the column for data)
    pivot_longer(starts_with("d_"), names_to = "date", values_to = "sales") %>%
    mutate(date = as.integer(str_remove(date, 'd_'))) %>% #This removes the d_
    mutate(date = min_date + date - 1)

}

# Test the function
# s_df = head(train, n=50)
# column = c('id', 'state_id')
# extract_ts(s_df, column)

# Create a function to form the train data, group by wm_wk_yr, merge with price data
get_rev2 = function(df_tr, df_pr, df_wk){
  df_tr = df_tr %>%
    group_by(item_id, store_id)

  df_tr = extract_ts(df_tr, c('item_id', 'cat_id', 'store_id'))
  print('extracted')

  #merge with weekly to wm_wk_yr
  df_tr = inner_join(df_wk[,c('date','wm_yr_wk')],df_tr, by = 'date')
  print('merged with week')

  #Group by wm_wk_yr and sum the sales
  df_tr = df_tr %>%
    group_by(wm_yr_wk, store_id, cat_id, item_id) %>%
    summarise(sales = sum(sales), yr = mean(year(date)))
  print('grouped by week')

  #merge with prices
  df_tr = inner_join(df_tr, df_pr, by = c('store_id', 'item_id',
                                                   'wm_yr_wk'))
  print('merged with prices')

  #get Revenue for each item
  df_tr$revenue = df_tr$sales*df_tr$sell_price
  print('got revenue')

  #Change order of the columns
  df_tr = df_tr[c('item_id', 'cat_id', 'store_id', 'wm_yr_wk', 'yr', 'sales', 'sell_price',
                          'revenue')]
}

#CORRELATION BETWEEN NORMAL AND NONNORMAL DATA
corr.normal = function(vector){
  X = sort(vector, decreasing = FALSE)
  n = length(X)
  prob = (c(1:n)-.5)/n
  Z = qnorm(prob)
  
  return(cor(X,Z))
}

#wILCOX TEST BETWEEN GROUPS
paired.wilcox = function(df, p_adjust){
  #note: Factor first column, values second column
  #note: Use p_adjust = TRUE for adjusting the p-value with bonferroni
  #change the colnames for easy vector use
  colnames(df) = c('Factor', 'Value')
  #get the factors for loop
  factors = unique(df$Factor)
  n = length(factors)

  mat = matrix(nrow = n-1, ncol = n-1)
  colnames(mat) = factors[1:n-1]
  rownames(mat) = factors[2:n]
  pvalue = c()
  tracker = c()
  count = 1
  for (i in factors){
    if (count > 1){
      new_factors = factors[-tracker]
    }
    else{
      new_factors = factors
    }
    for (j in new_factors[-1]){
      X = df[df$Factor == i,]$Value
      Y = df[df$Factor == j,]$Value
      mat[j,i] = wilcox.test(X,Y)$p.value
    }
    tracker = c(tracker, count)
    count = count + 1
  }
  #adjust p-value if wanted
  if (p_adjust){
    mat = matrix(p.adjust(mat), nrow = n-1, ncol = n-1, byrow = FALSE)
    colnames(mat) = factors[1:n-1]
    rownames(mat) = factors[2:n]
    return(mat)
  }
  else{
    return(mat)
  }
}
```

### Import the Data
```{r, echo=FALSE, warning=FALSE}
setwd("Z:/Data Science/Spring Semester/Applied Statistics and Multivariate Analysis/Project/Data")

#Import data
train = vroom('sales_train_validation.csv', delim = ",", col_types = cols())
prices = vroom('sell_prices.csv', delim = ",", col_types = cols())
calendar = vroom('calendar.csv', delim = ",", col_types = cols())
```

#### Train Data
```{r, echo=FALSE}
column_names = c(names(train[,1:8]),'....',names(train[,1915:1919]))
dim = dim(train)
```
The train data has a size of `r dim` with the following names:  
`r column_names`  
Looking at this, I initially see that we will need to pivot the day columns in order to make it workeable.  
  
Here is a quick look at the top 5 rows for some of the columns.  
```{r, echo=FALSE}
kable(cbind(train[1:5,2:7], train[1:5,1915:1919]))
```

Looking at the unique values in the qualitative columns of the dataframe, shown in the table below, you can kind of get an idea of what this dataframe is telling us. You can see that there are 3049 items spread out through 10 stores in 3 different states. A lot of these items are repeated through the different stores. You can also see that in each store there are 3 categories with 7 departments.  
```{r, echo=FALSE}
#Get unique values in qualitative values
column_names = names(train[,2:6])
train_counts = matrix(nrow = 1, ncol = 5, dimnames = list('unique', column_names))
for (i in column_names){
  train_counts[,i] = n_distinct(train[,i])
}
kable(train_counts)
```

The table below is the distribution of 3 random days for all the items through all of the stores and categories. With the Median value being 0, you can see that a lot of these values are zero. This is because they will not always sell all of the items all of the time, or some may have been discontinued. The amount of zeros doesn't pose much of a threat since we are aggregating over different variables and time.  
```{r, echo = FALSE}
column_names = sample(names(train[,7:1919]) , size = 3)
kable(stats(train[,column_names]))
```
#### Prices Data
```{r, echo = FALSE}
column_names = names(prices)
dim = dim(train)
```
The prices data frame has a size of `r dim` with the following column names:  
`r column_names`  
You can see that there are a couple of rows that tie this data frame to the train data frame, which are `r column_names[1:2]`.  
  
Here is a quick look at the head of the dataframe.  
```{r, echo=FALSE}
kable(rbind(head(prices)))
```

The only new real interesting variable we get from this dataframe is the "sell_price" variable, which we can use to calculate revenue later. The sell price is given as a weekly average of the price for that item at that store. The stats and box-plot of this value are shown below.  
```{r, echo = FALSE}
kable(stats(prices[,'sell_price']))
boxplot(prices[,'sell_price'], main = 'Sell Price', ylab = 'Dollars')
```

Looking at the box plot, you can see that a lot of the item prices are pretty low with a select few of them reaching over 40 dollars.  
  
#### Calendar Data
The columns in the calendar data are as follows:  
```{r, echo=FALSE}
column_names = names(calendar)
dim = dim(calendar)
```  
The calendar data frame has a size of `r dim` with the following column names:  
`r column_names`  
The column `r column_names[7]` is the column name that ties this dataset back to the train dataset.  
  
By looking at the first and last values in the data frame, we can see that the time frame from 2011-01-29 to 2016-06-19.  
# ```{r, echo=FALSE}
# kable(rbind(head(calendar, n= 3),tail(calendar, n=3)))
# ```  

This data set also gives you data on whether there is an event and what that event type is. It also gives you information on whether that day is a SNAP day or not in either of the three states. The number of Holidays and Holiday types are displayed in the table below.    
```{r, echo=FALSE}
#Get unique values in qualitative values
column_names = names(calendar[,8:14])
calendar_counts = matrix(nrow = 2, ncol = length(column_names), 
                      dimnames = list(c('count','unique'), column_names))
for (i in column_names){
  calendar_counts['unique',i] = n_distinct(calendar[,i])
  if (i == column_names[5] || i == column_names[6] || i == column_names[7]){
    calendar_counts['count',i] = sum(calendar[,i])
  }
  else{
    calendar_counts['count',i] = dim[1]-sum(is.na(calendar[,i]))
  }
}
kable(calendar_counts)
```  
  
There are not a lot of events throughout the year that this keeps track of, just 31 unique events in all of the 5 plus years. Each state only has a total of 650 SNAP days because they are required to have so many in 1 year.  
  
The "wm_yr_wk" column tracks the number of weeks in a year but it does it in a tricky way. It start the 1st week on the first day of data collection, and starts it count there. So week 52 of the first year will actually be around 2012-01-30. This can be seen with the following table, which looks at the 1st 5 values of the first year and the second year.  
  
```{r, echo=FALSE, warning=FALSE}
ind = c(calendar['wm_yr_wk']<=11201)
#c(min(calendar[ind, 'wm_yr_wk']), max(calendar[ind, 'wm_yr_wk']))

# wm_yr_wk = 1_yr starts_week of relative year
kable(
  rbind(
  cbind(head(calendar[ind, 'date'], n = 5), head(calendar[ind, 'wm_yr_wk'], n= 5)),
  cbind(tail(calendar[ind, 'date'], n = 5), tail(calendar[ind, 'wm_yr_wk'], n = 5))
  ))
```

```{r, echo=FALSE, warning=FALSE}
#Weekly dataframe for merging with groups
weekly = calendar[,c('date','wm_yr_wk')]

week = c()
for (i in calendar$wm_yr_wk){
  if (i%%11100 < 54){
    week = append(week, i%%11100)
  }
  else if(i%%11200 < 54){
    week = append(week, i%%11200)
  }
  else if(i%%11300 < 54){
    week = append(week, i%%11300)
    }
  else if(i%%11400 < 54){
    week = append(week, i%%11400)
  }
  else if(i%%11500 < 54){
    week = append(week, i%%11500)
  }
  else if(i%%11600 < 54){
    week = append(week, i%%11600)
  }
  else{
    print(i)
  }
}

weekly = cbind(weekly, week)
```  
  
## Exploratory Analysis  
By aggregating all of the sales and pivoting the training table we can see the amount sales for each day in all 10 of the stores. The following time series chart shows how the total sales changes per day through out this time frame.  
```{r, echo=FALSE, warnings = FALSE}
total_sales <- train %>%
  summarise_at(vars(starts_with("d_")), sum) %>%
  mutate(id = 1)

total_sales = extract_ts(total_sales, c())
plot(x = total_sales$date, y = total_sales$sales, type = 'l', col = 'blue',
     main = 'Total Sales by Date')

```  
  
You can see the seasonality in the sales of these stores of where the shopping dies down at the end and begging of the years and peaks kind of late middle of the year. The drops at the end of every year are Christmas where the stores are closed for part of the day.  
  
We can also look at this weekly instead of daily to reduce the noise by summing up the total sales in a week, which is displayed below.    

```{r, echo=FALSE}
sales_weekly = inner_join(weekly, total_sales, by = 'date')

#groupby week and get the mean of that week for sales and start date
sales_weekly = sales_weekly %>%
  group_by(wm_yr_wk) %>%
  summarize(sales = sum(sales), date = min(date))

plot(x = sales_weekly$date, y = sales_weekly$sales, type = 'l', col = 'blue', main = 'Sales Time Series - Weekly Sum')
```  
  
One thing we can dive deeper into is how the sales correlates to different aspects of times, such as year, month, week, week day. The following is the correlation between sales and those factors.  
  
```{r, echo = FALSE}
#Add columns that give the year values, month values, week values
total_sales = total_sales %>%
  mutate(year = as.numeric(format(date, "%Y")),
         month = as.numeric(format(date, "%m")))
#add in the week
total_sales = inner_join(total_sales, weekly[,c('date','week')], by = 'date')
#add in wday and weekday
total_sales = inner_join(total_sales, calendar[,c('date','wday', 'weekday')], by = 'date')
total_sales$weekend <- ifelse(total_sales$wday <=2 , 1, 0)

#verify names
#names(total_sales)

#correlation plot of those time frames
sales_time_cor = cor(total_sales[,c('sales','year','month', 'week', 'wday')])[,'sales']
kable(sales_time_cor, col.names = 'r')
```  
  
The two factors that correlate the most with sales are year and wday, and even those are not great. Month and week number in the year have pretty much no correlation.  
  
### Comparison of Midweek Sales vs. Weekend Sales
The following is a box plot for midweek and weekend sales. It appears that there are more sales on Saturday and Sunday compared to the 5 other days.  
```{r}
week_comparison = total_sales[,c('sales','weekend')]
week_comparison$day_of_week <- ifelse(week_comparison$weekend == 0 , 'Midweek', 'Weekend')

ggplot(week_comparison, aes(x=as.factor(day_of_week), y=sales, fill = as.factor(day_of_week))) +   geom_boxplot() +
  theme( legend.position = 'none') +
  ggtitle('Midweek/Weekend Sales BoxPlot') +
  xlab('')
```  
  
One of our questions of interest was if weekeend sales differed from weekday sales. Leading us to the following hypothesis:  
H0: Mean sales for the weekday = Mean sales for the weekend  
Ha: Mean sales for weekday != Mean sales for the weekend  
  
The p-value is <.05 so we reject the null hypothesis and claim that there is a difference in sales between midweek days and the weekends.  
```{r}
weekend_sales = total_sales$sales[total_sales$weekend == 1]
midweek_sales = total_sales$sales[total_sales$weekend == 0]
kruskal.test(total_sales$sales, total_sales$weekend)
```  
```{r, echo=FALSE}
#wkend.fit = anova(lm(sales ~ weekend, data = total_sales))
#wkend.fit
```  
  
 The results from this test shows that we have enough evidence to support our claim that there is a difference between the sales in the weekend and midweek. But which days have the least amount of sales and which days have the most?
  
### Comparison of Sales and Weekday  
The following plot is a box plot of the days in the week and sales. By looking at this plot, it appears that there are less sales between Tuesday, Wednesday, and Thursday than in the other days of the week. It also appears that the most shopping is done on Saturday and Sunday.  
  
```{r, echo= FALSE}
ggplot(total_sales, aes(x=as.factor(weekday), y=sales, fill = as.factor(weekday))) +   geom_boxplot() +
  theme( legend.position = 'none') +
  ggtitle('Weekday BoxPlots') +
  xlab('')
```  
  
This plot leads us to the following hypothesis:  
$H_{0}: \mu_{Mon} = \mu_{Tue} = \mu_{Wed} = \mu_{Thu} = \mu_{Fri} = \mu_{Sat} = \mu_{Sun}$  
$H_{a}: \mu_{i} \neQ \mu_{j}$ for any i,j in Mon, Tue, Wed, Thu, Fri, Sat, Sun where $i \ne j$  
  
The below table is the descriptive stats for the days in the week.  
  
```{r, echo=FALSE}
wday.stats = cbind(tapply(total_sales$sales, total_sales$weekday, mean),
              tapply(total_sales$sales, total_sales$weekday, sd),
              tapply(total_sales$sales, total_sales$weekday, length),
              tapply(total_sales$sales, total_sales$weekday, median))
colnames(wday.stats) = c('Mean', 'Std_Dev', 'Count', 'Median')
```  
  
```{r, echo = FALSE}
#QQ plot - got a few outliers
qqnorm(total_sales$sales)
qqline(total_sales$sales)
#Can see the outliers really well
hist(total_sales$sales)
#Shapiro Test for normality
shapiro.test(total_sales$sales)
```  
  
```{r, echo=FALSE}  
#remove the outliers
index = which(total_sales$sales < 10000)
X = total_sales[-index, c('sales', 'weekday')]
#QQ plot - looks better (normal??)
qqnorm(X$sales)
qqline(X$sales)
#Histogram
hist(X$sales)
#Shapiro test for normality - still not normal
shapiro.test(X$sales)
```  
  
```{r, echo = FALSE}
#do a correlation test
wday.normcorr = corr.normal(X$sales)
```  
  
```{r, echo = FALSE}
#Kruskal-Wallis Test for non-normal data
kruskal.test(X$sales, X$weekday)
#There is difference between the weekdays
```  
  
```{r, echo=FALSE}
#Do the wilcoxon test between all of the pairs
#adjusted using bonferroni method
wday.wilcox = paired.wilcox(total_sales[,c('weekday', 'sales')], p_adjust = TRUE)
wday.wilcox
```  
  
#### Comparison of Sales and Event Type
``` {r, echo = FALSE}
#There are 4 unique holidays and snap day for each location

#Get all of the dates for the holidays
ind = calendar$event_type_1 == 'Sporting'
sporting = calendar[which(ind == TRUE),c('date','event_type_1' )]

ind = calendar$event_type_1 == 'Cultural'
cultural = calendar[which(ind == TRUE),c('date','event_type_1' )]

ind = calendar$event_type_1 == 'National'
national = calendar[which(ind == TRUE),c('date','event_type_1' )]

ind = calendar$event_type_1 == 'Religious'
religious = calendar[which(ind == TRUE),c('date','event_type_1' )]

#joing them together
events = rbind(
inner_join(total_sales[,c('date', 'sales')], sporting, by = 'date'),
inner_join(total_sales[,c('date', 'sales')], cultural, by = 'date'),
inner_join(total_sales[,c('date', 'sales')], national, by = 'date'),
inner_join(total_sales[,c('date', 'sales')], religious, by = 'date')
)

#Join them will all of the daily sales
sales.events = full_join(total_sales[,c('date', 'sales')], events, by = c('date', 'sales'))
sales.events[is.na(sales.events$event_type_1), 'event_type_1'] = 'nothing'

#We already know that the sales data is not normal from earlier studies
#Kruskal-Wallis Test for non-normal data
kruskal.test(sales.events$sales, sales.events$event_type_1)
#There is difference between the weekdays
```  
  
```{r, echo=FALSE}
#Do the wilcoxon test between all of the pairs
#no adjustment
paired.wilcox(sales.events[,c('event_type_1', 'sales')], p_adjust = FALSE)
```  

```{r, echo = FALSE}
#adjusted using bonferroni method
paired.wilcox(sales.events[,c('event_type_1', 'sales')], p_adjust = TRUE)
```  
  
#### Look at Sales and Year  
  
This next chart shows the box plots of the sales through each year in the dataframe.  
```{r, echo= FALSE}
ggplot(total_sales, aes(x=as.factor(year), y=sales, fill = as.factor(year))) +   geom_boxplot() +
  theme( legend.position = 'none') +
  ggtitle('Year BoxPlots') +
  xlab('')
```  
  
This appears to show that there is a steady increase in sales as the years progress. The next graph shows a time series of sales by each category: food, household, and hobbies.  
  
```{r, echo=FALSE}
category_sales = train %>%
  group_by(cat_id) %>%
  summarise_at(vars(starts_with("d_")), sum)

category_sales = extract_ts(category_sales, c('cat_id'))

#Merge data to get week relative toyear
category_sales = inner_join(weekly, category_sales, by = 'date')

#groupby week and get the mean of that week for sales and start date
category_weekly = category_sales %>%
  group_by(wm_yr_wk, cat_id) %>%
  summarize(sales = sum(sales), date = min(date))

#Plot the Data
ggplot(data=category_weekly, aes(x=date, y=sales, col = cat_id)) + geom_line() + ggtitle('Sales by Category')
```  
  
### Comparison of States  
The next graph shows a time series of sales by each category: California, Texas, and Wisconsin.  

```{r, echo=FALSE}
state_sales = train %>%
  group_by(state_id) %>%
  summarise_at(vars(starts_with("d_")), sum)

#Merge with Weekly Data
state_sales = extract_ts(state_sales, c('state_id'))
state_sales = inner_join(weekly, state_sales, by = 'date')

#Groupby Weekly
state_weekly = state_sales %>%
  group_by(wm_yr_wk, state_id) %>%
  summarise(sales= sum(sales), date = min(date))

#Plot
ggplot(data=state_weekly, aes(x=date, y=sales/1000, col = state_id)) + geom_line() + ggtitle('Sales by State (in thousands)')

```

Looking at the graph, it almost appears that every state has its own rate for increase in sales per time. It appears that CA and WI may be similar but TX seems to be less steep than the other two. We will also like to compare the rates of sales increase in each step by looking at the followinig.  

$H_{0}: \beta_{1,CA} = \beta_{1,TX} = \beta_{1,WI}$  
$H_{a}: \beta_{1,i} \neq \beta_{1,j} ~~\forall~~i,j = CA, TX, WI~~and~~i \neq j$  
  
The following is a look at weekly sales by state. First is CA.

```{r, echo=FALSE}
#California weekly sales

train_CA <- train[train$state_id == 'CA',]

CA_sales = train_CA %>%
  group_by(state_id) %>%
  summarise_at(vars(starts_with("d_")), sum)

#Merge with Weekly Data
CA_sales = extract_ts(CA_sales, c('state_id'))
CA_sales= inner_join(weekly, CA_sales, by ='date')

#Groupby Weekly
CA_weekly = CA_sales %>%
  group_by(wm_yr_wk, state_id) %>%
  summarise(sales= sum(sales), date = min(date))

#Plot
ggplot(data=CA_weekly, aes(x=date, y=sales/1000, col = state_id)) + geom_line() + ggtitle('CA Sales')
```

A linear regression gives us our $\beta_{0}$ and $\beta{1}$ coefficents, as displayed in the following table.  

```{r, echo = FALSE}
summary(CA.lm <- lm(CA_weekly$sales ~ CA_weekly$date, data=CA_weekly))
```

We can also see our fitted line with the following chart.  

```{r, echo = FALSE}
plot(CA_weekly$date,CA_weekly$sales, pch = 16, cex = 0.7, col = "blue", main = 'CA Sales', xlab= 'date', ylab = 'sales')
abline(lm(CA_weekly$sales ~ CA_weekly$date))
```

The following two plot shows that CA sales are normally distributed with a fairly equal variance. Index 274 seems to be a bit of an outlier, which may need further investigation.  

```{r, echo=FALSE}
plot(CA.lm, which = 2) #Normal Q-Q
plot(CA.lm, which = 1)
```

Next is a look at weekly sales in Texas.

```{r, echo = FALSE}
#Texas Sales
train_TX = train[train$state_id == 'TX',]

TX_sales = train_TX %>%
  group_by(state_id) %>%
  summarise_at(vars(starts_with("d_")), sum)

#Merge with Weekly Data
TX_sales = extract_ts(TX_sales, c('state_id'))
TX_sales= inner_join(weekly, TX_sales, by ='date')

#Groupby Weekly
TX_weekly = TX_sales %>%
  group_by(wm_yr_wk, state_id) %>%
  summarise(sales= sum(sales), date = min(date))

#Plot
ggplot(data=TX_weekly, aes(x=date, y=sales, col = state_id)) + geom_line() + ggtitle('TX Sales')

```

The following is the linear model table for Texas.  

```{r, echo = FALSE}
summary(TX.lm <- lm(TX_weekly$sales ~ TX_weekly$date, data=TX_weekly))
```

You can see that Texa's slope is `r as.numeric(TX.lm$coef[2])` and California's slope is `r as.numeric(CA.lm$coef[2])`.  
  
The following is the plot of the fitted line. 

```{r, echo = FALSE}
plot(TX_weekly$date,TX_weekly$sales, pch = 16, cex = 0.7, col = "blue", main = 'TX Sales', xlab= 'date', ylab = 'sales')
abline(lm(TX_weekly$sales ~ TX_weekly$date))
```

The following two plot shows that CA sales are normally distributed with a fairly equal variance. Index 274 seems to be a bit of an outlier again, which may need further investigation.   

```{r, echo = FALSE}
plot(TX.lm,which = 2) #Normal Q-Q
plot(TX.lm,which = 1) #Normal Q-Q
```

The following is a look at Wisconsin's weekly sales.  

```{r, echo = FALSE}
#Wisconsin Sales
train_WI = train[train$state_id == 'WI',]

WI_sales = train_WI %>%
  group_by(state_id) %>%
  summarise_at(vars(starts_with("d_")), sum)

#Merge with Weekly Data
WI_sales = extract_ts(WI_sales, c('state_id'))
WI_sales= inner_join(weekly, WI_sales, by ='date')

#Groupby Weekly
WI_weekly = WI_sales %>%
  group_by(wm_yr_wk, state_id) %>%
  summarise(sales= sum(sales), date = min(date))

#Plot
ggplot(data=WI_weekly, aes(x=date, y=sales, col = state_id)) + geom_line() + ggtitle('WI Sales')

```

The following is the linear model table for Wisconsin.  

```{r}
summary(WI.lm <- lm(WI_weekly$sales ~ WI_weekly$date, data=WI_weekly))
```

You can see that Texas' slope is `r as.numeric(TX.lm$coef[2])`, California's slope is `r as.numeric(CA.lm$coef[2])`, and Wisconsin's slope is `r as.numeric(WI.lm$coef[2])`.   
  
The following is plot of the fitted line.  

```{r, echo = FALSE}
plot(WI_weekly$date,TX_weekly$sales, pch = 16, cex = 0.7, col = "blue", main = 'WI Sales', xlab= 'date', ylab = 'sales')
abline(lm(WI_weekly$sales ~ WI_weekly$date))
```

The following two plot shows that CA sales are normally distributed with a fairly equal variance. Index 274 seems to be a bit of an outlier again, which may need further investigation.   

```{r, echo = FALSE}
plot(WI.lm,which = 2) #Normal Q-Q
plot(WI.lm,which = 1)
```
The next step will be actually doing the Hypothesis test on the difference in the $\beta_{1}$ coefficients for each of these states.  
  
### Comparison of Category Sales by State
The next thing we want to look at is how the sales and revenue of the ten stores compares to eachother.
# ```{r, echo = FALSE}
# ind = train['state_id'] == 'CA'
# cali_rev = train[ind,]
# 
# ind = train['state_id'] == 'TX'
# tex_rev = train[ind,]
# 
# ind = train['state_id'] == 'WI'
# wis_rev = train[ind,]
# 
# wis_rev2 = get_rev2(wis_rev, prices, weekly)
# tex_rev2 = get_rev2(tex_rev, prices, weekly)
# cali_rev2 = get_rev2(cali_rev, prices, weekly)
# 
# setwd("Z:/Data Science/Spring Semester/Applied Statistics and Multivariate Analysis/Project/Data")
# #Export to excel
# vroom_write(wis_rev2, 'wis_rev2.csv', ',')
# vroom_write(tex_rev2, 'tex_rev2.csv', ',')
# vroom_write(cali_rev2, 'cali_rev2.csv', ',')
# ```  

```{r, echo=FALSE, warning=FALSE}
setwd("Z:/Data Science/Spring Semester/Applied Statistics and Multivariate Analysis/Project/Data")

wis_rev = vroom('wis_rev2.csv', delim = ',', col_types = cols())
tex_rev = vroom('tex_rev2.csv', delim = ',', col_types = cols())
cali_rev = vroom('cali_rev2.csv', delim = ',', col_types = cols())
```  
  
```{r, echo = FALSE}
#Get the store revenues and sales by category

#Group by year and get the sums
wis_stores = wis_rev %>%
  group_by(yr, cat_id, store_id) %>%
  summarise(sales = sum(sales), revenue = sum(revenue), avg_price = sum(revenue/sales))
#Group by cat_id and store_id and the means for average year sales and revenue
wis_stores = wis_stores %>%
  group_by(cat_id, store_id) %>%
  summarise(sales = mean(sales), revenue = mean(revenue), avg_price = mean(revenue/sales))

#Group by year and get the sums
tex_stores = tex_rev %>%
  group_by(yr, cat_id, store_id) %>%
  summarise(sales = sum(sales), revenue = sum(revenue), avg_price = sum(revenue/sales))
#Group by cat_id and store_id and the means for average year sales and revenue
tex_stores = tex_stores %>%
  group_by(cat_id, store_id) %>%
  summarise(sales = mean(sales), revenue = mean(revenue), avg_price = mean(revenue/sales))

#Group by year and get the sums
cali_stores = cali_rev %>%
  group_by(yr, cat_id, store_id) %>%
  summarise(sales = sum(sales), revenue = sum(revenue), avg_price = sum(revenue/sales))
#Group by cat_id and store_id and the means for average year sales and revenue
cali_stores = cali_stores %>%
  group_by(cat_id, store_id) %>%
  summarise(sales = mean(sales), revenue = mean(revenue), avg_price = mean(revenue/sales))

#combine them
stores.cat = rbind(cali_stores,
               tex_stores,
               wis_stores)

#What the average category sales, makes, price is for each of the stores.
stores_stats = stats(stores.cat)
```  
  
```{r, echo = FALSE}
# Look at average prices in each category for each store

#unique stores
groups = unique(stores.cat$store_id)

#set up storage matrix
cat_prices = matrix(nrow = 10, ncol = 3)
row.val = 1
#for loop to go through them
for (i in groups){
  inf = stores.cat[stores.cat$store_id == i, ]$avg_price
  cat_prices[row.val, ] = inf
  row.val = row.val + 1
}
colnames(cat_prices) = c('Food', 'Hobbies', 'Household')

#groupby store - drop cat
stores_price = stores.cat %>%
              group_by(store_id) %>%
              summarise( sales = sum(sales), revenue = sum(revenue), avg_price = mean(avg_price) )

#join them on store
stores_price = cbind(stores_price, cat_prices)
stores_price
```  
  
The following two tables show the average yearly sales and revenue for each store throughout the entire time frame and the basic descriptive statics for sales, revenue, and average item price. 
  
```{r, echo = FALSE}
kable(stores_price[,c('store_id', 'sales','revenue')])
kable(stores_stats)
```  
  
The following graph show the scatter plot with box plots of this data. Our next steps will be to do a multivariate analysis of the data and see if we find anything significant.  
  
```{r, echo=FALSE, warning=FALSE}
#Creating Scatterplot with Boxplot
par(fig=c(0,0.8,0,0.8), new=TRUE)
plot(stores_price$sales/1000000, stores_price$revenue/1000000, xlab="Sales", ylab="Revenue")
par(fig=c(0,0.8,0.5,1), new=TRUE)
boxplot(stores_price$sales/1000000, horizontal=TRUE, axes=FALSE)
par(fig=c(0.65,1,0,0.8),new=TRUE)
boxplot(stores_price$revenue/1000000, axes=FALSE)
mtext("Revenue vs. Sales (in millions)", side=3, outer=TRUE, line=-3)
```  
  
```{r, echo = FALSE}
#install.packages('biotools')
library('biotools')
#install.packages('MVN')
library('MVN')

#Multivariate Normal Test
mvn(stores_price[,c('sales', 'revenue', 'Food', 'Hobbies', 'Household')], multivariatePlot = 'qq')

#Box M test 
#boxM( stores_price[,c('Food', 'Hobbies', 'Household')], stores_price[,'state'])

#(3 variables and 3 stores for some states - need to do batlett's test for each one)
bartlett.test(Food ~ state, data = stores_price)
bartlett.test(Hobbies ~ state, data = stores_price)
bartlett.test(Household ~ state, data = stores_price)
```  
   
```{r, echo = FALSE}
#get all of the states
state = c()
for (i in groups){
  state = append(state, strsplit(i, split = '_')[[1]][1])
}
#attach state to stores_price
stores_price = cbind(state, stores_price)

#connection between store and store price?
attach(stores_price)

Y = cbind(Food, Hobbies, Household)
fit.category = manova(Y ~ state)
summary(fit.category)

detach(stores_price)
```  
  
```{r, echo = FALSE}
summary(fit.category, test = 'Wilks')
```  
  
``` {r, echo = FALSE}
library('profileR')
#How do the prices for each category differ per each state

#get the means in columns
columns = c('Food', 'Hobbies', 'Household')
means = rbind(  colMeans(stores_price[stores_price$state == 'CA', columns]),
                colMeans(stores_price[stores_price$state == 'TX', columns]),
                colMeans(stores_price[stores_price$state == 'WI', columns])
              )
rownames(means) = c('CA', 'TX', 'WI')

#profile plot
profileplot(means,rownames(means),standardize = F)
```  
  
```{r, echo = FALSE}
# Look at box plot of these values
attach(stores_price)
par(mfrow = c(1,3))
boxplot(Food ~ state)
boxplot(Hobbies ~ state)
boxplot(Household ~ state)

detach(stores_price)
```  
  
```{r, echo = FALSE}
#Are all three price portfolios different from each other?
attach(stores_price)

#Individual Anovas
a.food = anova(lm(Food ~ state,data = stores_price))
a.hobbies = anova(lm(Hobbies ~ state,data = stores_price))
a.household = anova(lm(Household ~ state,data = stores_price))

#p-values
pvalue = c(a.food$`Pr(>F)`[1],a.hobbies$`Pr(>F)`[1],a.household$`Pr(>F)`[1])
#pvalue

#adjust p-values
p.adjust(pvalue,method = "bon")
padj = cbind(c("Food", 'Hobbies', 'Household'),round(p.adjust(pvalue,method = "bon"), 3))
colnames(padj) = c('Category', 'Adjust P-Value')
kable(padj)
```  
  
#### References
[1] https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda

